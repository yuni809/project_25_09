{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1f7b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: openai-whisper in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: librosa in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: soxr in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (2.2.6)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (0.12.0)\n",
      "Requirement already satisfied: torch in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from cffi>=1.0->soundfile) (2.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.19.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: mediapipe==0.10.14 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: deepface in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.0.95)\n",
      "Requirement already satisfied: mtcnn in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: retina-face in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.0.17)\n",
      "Requirement already satisfied: absl-py in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (25.9.23)\n",
      "Requirement already satisfied: jax in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (3.10.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (2.2.6)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (4.12.0.88)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.14)\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe==0.10.14) (0.5.3)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (2.32.5)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (2.3.3)\n",
      "Requirement already satisfied: gdown>=3.10.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (5.2.0)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (4.67.1)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (12.0.0)\n",
      "Requirement already satisfied: tensorflow>=1.9.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (2.20.0)\n",
      "Requirement already satisfied: keras>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (3.12.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (3.1.2)\n",
      "Requirement already satisfied: flask-cors>=4.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (6.0.1)\n",
      "Requirement already satisfied: fire>=0.4.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (0.7.1)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface) (23.0.0)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mtcnn) (1.5.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mtcnn) (4.4.5)\n",
      "Requirement already satisfied: termcolor in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from fire>=0.4.0->deepface) (3.2.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (8.3.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (2.1.5)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from click>=8.1.3->Flask>=1.1.2->deepface) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gdown>=3.10.1->deepface) (4.14.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gdown>=3.10.1->deepface) (3.19.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gunicorn>=20.1.0->deepface) (25.0)\n",
      "Requirement already satisfied: rich in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras>=2.2.0->deepface) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras>=2.2.0->deepface) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras>=2.2.0->deepface) (3.15.1)\n",
      "Requirement already satisfied: optree in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras>=2.2.0->deepface) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras>=2.2.0->deepface) (0.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->deepface) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface) (2025.11.12)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe==0.10.14) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.14) (2.23)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow>=1.9.0 (from deepface)\n",
      "  Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (80.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.76.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow>=1.9.0->deepface)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting numpy (from mediapipe==0.10.14)\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=1.9.0->deepface)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->deepface) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->deepface) (0.7.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.8)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jax->mediapipe==0.10.14) (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe==0.10.14) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe==0.10.14) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe==0.10.14) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe==0.10.14) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe==0.10.14) (3.2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from rich->keras>=2.2.0->deepface) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from rich->keras>=2.2.0->deepface) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
      "Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "   ---------------------------------------- 0.0/375.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/375.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/375.7 MB 9.4 MB/s eta 0:00:40\n",
      "    --------------------------------------- 8.7/375.7 MB 17.9 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 16.5/375.7 MB 24.2 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 24.1/375.7 MB 26.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 32.5/375.7 MB 29.9 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 32.8/375.7 MB 27.0 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 37.7/375.7 MB 25.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 38.8/375.7 MB 22.8 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 41.2/375.7 MB 21.1 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 48.5/375.7 MB 22.4 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 55.3/375.7 MB 23.3 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 62.9/375.7 MB 24.5 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 71.0/375.7 MB 25.5 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 78.9/375.7 MB 26.4 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 86.2/375.7 MB 26.8 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 90.2/375.7 MB 27.3 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 96.7/375.7 MB 26.6 MB/s eta 0:00:11\n",
      "   ---------- ---------------------------- 103.8/375.7 MB 27.2 MB/s eta 0:00:11\n",
      "   ----------- --------------------------- 107.5/375.7 MB 26.5 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 115.9/375.7 MB 27.1 MB/s eta 0:00:10\n",
      "   ------------ -------------------------- 125.0/375.7 MB 27.9 MB/s eta 0:00:09\n",
      "   ------------- ------------------------- 133.7/375.7 MB 28.5 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 141.6/375.7 MB 28.9 MB/s eta 0:00:09\n",
      "   --------------- ----------------------- 149.9/375.7 MB 29.3 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 158.9/375.7 MB 29.9 MB/s eta 0:00:08\n",
      "   ----------------- --------------------- 167.2/375.7 MB 30.2 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 172.8/375.7 MB 30.0 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 181.4/375.7 MB 30.4 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 190.6/375.7 MB 30.8 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 198.4/375.7 MB 31.0 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 207.4/375.7 MB 31.4 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 210.2/375.7 MB 30.8 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 218.6/375.7 MB 31.1 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 227.0/375.7 MB 31.3 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 235.7/375.7 MB 31.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 236.5/375.7 MB 30.9 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 242.5/375.7 MB 30.8 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 249.8/375.7 MB 30.8 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 257.7/375.7 MB 31.0 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 265.6/375.7 MB 31.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 273.9/375.7 MB 32.1 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 282.6/375.7 MB 32.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 291.0/375.7 MB 32.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 299.1/375.7 MB 33.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 302.0/375.7 MB 34.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 310.4/375.7 MB 34.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 317.7/375.7 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 320.1/375.7 MB 33.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 325.6/375.7 MB 33.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 332.1/375.7 MB 33.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 338.7/375.7 MB 33.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 339.5/375.7 MB 32.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 349.2/375.7 MB 32.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 358.6/375.7 MB 33.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.1/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  369.1/375.7 MB 33.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 33.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 375.7/375.7 MB 29.2 MB/s  0:00:13\n",
      "Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 6.6/12.9 MB 33.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/12.9 MB 23.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 23.7 MB/s  0:00:00\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 30.5 MB/s  0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.0 MB/s  0:00:00\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, protobuf, numpy, tensorboard, tensorflow\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 6.33.1\n",
      "\n",
      "    Uninstalling protobuf-6.33.1:\n",
      "\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "      Successfully uninstalled protobuf-6.33.1\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "  Attempting uninstall: numpy\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "    Found existing installation: numpy 2.2.6\n",
      "   -------- ------------------------------- 1/5 [protobuf]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "    Uninstalling numpy-2.2.6:\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "  Attempting uninstall: tensorboard\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "    Found existing installation: tensorboard 2.20.0\n",
      "   ---------------- ----------------------- 2/5 [numpy]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "    Uninstalling tensorboard-2.20.0:\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "      Successfully uninstalled tensorboard-2.20.0\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "  Attempting uninstall: tensorflow\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "    Found existing installation: tensorflow 2.20.0\n",
      "   ------------------------ --------------- 3/5 [tensorboard]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "    Uninstalling tensorflow-2.20.0:\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "      Successfully uninstalled tensorflow-2.20.0\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   -------------------------------- ------- 4/5 [tensorflow]\n",
      "   ---------------------------------------- 5/5 [tensorflow]\n",
      "\n",
      "Successfully installed numpy-2.1.3 protobuf-4.25.8 tensorboard-2.19.0 tensorflow-2.19.1 tensorflow-io-gcs-filesystem-0.31.0\n",
      "Requirement already satisfied: transformers==4.57.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from transformers==4.57.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tqdm>=4.27->transformers==4.57.1) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests->transformers==4.57.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests->transformers==4.57.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests->transformers==4.57.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests->transformers==4.57.1) (2025.11.12)\n",
      "Collecting openai\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.5-cp310-cp310-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.5 MB/s  0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 37.3 MB/s  0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "\n",
      "   ----------- ---------------------------- 2/7 [jiter]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------- ----- 6/7 [openai]\n",
      "   ---------------------------------------- 7/7 [openai]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.12.0 openai-2.8.1 pydantic-2.12.4 pydantic-core-2.41.5 typing-inspection-0.4.2\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# #    \n",
    "\n",
    "# # /\n",
    "# !pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121  # GPU\n",
    "# !pip install torch torchaudio\n",
    "\n",
    "# !pip install openai-whisper librosa soundfile soxr\n",
    "\n",
    "# # /\n",
    "# !pip install opencv-python mediapipe==0.10.14 deepface mtcnn retina-face\n",
    "\n",
    "# # NLP/\n",
    "# !pip install transformers==4.57.1 safetensors\n",
    "\n",
    "# # LLM (GPT-4 )\n",
    "# !pip install openai tiktoken\n",
    "\n",
    "# # \n",
    "# !pip install numpy pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39af98cf-d62f-48be-b360-21fe8ce28c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Owner\\anaconda3\\envs\\emotion\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Owner\\anaconda3\\envs\\emotion\\Lib\\site-packages\\~orchaudio'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Owner\\anaconda3\\envs\\emotion\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# # Torch, Torchaudio (CUDA 12.1  ) \n",
    "# !pip install -q torch==2.3.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# # transformers, datasets, accelerate     ( )\n",
    "# !pip install -q transformers==4.41.2 datasets==2.19.1 evaluate==0.4.2 accelerate==0.30.1 librosa pandas gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61039cd7-4855-46b5-84c1-9d89390886d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (1.51.0)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: deepface==0.0.95 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.0.95)\n",
      "Collecting tensorflow==2.20.0\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting keras==3.12.0\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: opencv-python==4.12.0.88 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (4.12.0.88)\n",
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (2.3.3)\n",
      "Requirement already satisfied: gdown>=3.10.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (5.2.0)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (4.67.1)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (12.0.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (3.1.2)\n",
      "Requirement already satisfied: flask-cors>=4.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (6.0.1)\n",
      "Requirement already satisfied: mtcnn>=0.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (1.0.0)\n",
      "Requirement already satisfied: retina-face>=0.0.14 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (0.0.17)\n",
      "Requirement already satisfied: fire>=0.4.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (0.7.1)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from deepface==0.0.95) (23.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow==2.20.0)\n",
      "  Downloading protobuf-6.33.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (1.14.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (1.76.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow==2.20.0)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorflow==2.20.0) (3.15.1)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow==2.20.0)\n",
      "  Downloading ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras==3.12.0) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras==3.12.0) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from keras==3.12.0) (0.18.0)\n",
      "Collecting numpy>=1.14.0 (from deepface==0.0.95)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface==0.0.95) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface==0.0.95) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface==0.0.95) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.27.1->deepface==0.0.95) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0) (3.1.3)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (6.2.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (8.3.1)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface==0.0.95) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface==0.0.95) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pandas>=0.23.4->deepface==0.0.95) (2025.2)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from google-generativeai) (2.41.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from google-generativeai) (2.12.4)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow==2.20.0)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Collecting future (from ffmpeg-python)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.20.0) (0.45.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface==0.0.95) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from Flask>=1.1.2->deepface==0.0.95) (2.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gdown>=3.10.1->deepface==0.0.95) (4.14.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from gdown>=3.10.1->deepface==0.0.95) (3.19.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mtcnn>=0.1.0->deepface==0.0.95) (1.5.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mtcnn>=0.1.0->deepface==0.0.95) (4.4.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from beautifulsoup4->gdown>=3.10.1->deepface==0.0.95) (2.8)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface==0.0.95) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from rich->keras==3.12.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from rich->keras==3.12.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras==3.12.0) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp310-cp310-win_amd64.whl (331.7 MB)\n",
      "   ---------------------------------------- 0.0/331.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.4/331.7 MB 18.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 10.0/331.7 MB 24.9 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 18.1/331.7 MB 30.1 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 26.7/331.7 MB 33.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 35.7/331.7 MB 34.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 43.8/331.7 MB 35.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 52.7/331.7 MB 36.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 61.9/331.7 MB 37.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 71.8/331.7 MB 38.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 80.7/331.7 MB 38.7 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 88.6/331.7 MB 38.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 96.5/331.7 MB 38.5 MB/s eta 0:00:07\n",
      "   ------------ -------------------------- 104.6/331.7 MB 38.4 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 111.1/331.7 MB 37.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 119.5/331.7 MB 37.8 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 128.2/331.7 MB 38.1 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 136.3/331.7 MB 38.2 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 144.4/331.7 MB 38.1 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 153.4/331.7 MB 38.3 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 162.0/331.7 MB 38.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 164.9/331.7 MB 37.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 171.4/331.7 MB 36.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 180.4/331.7 MB 37.2 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 189.0/331.7 MB 37.3 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 197.7/331.7 MB 37.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 206.6/331.7 MB 37.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 215.7/331.7 MB 37.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 224.4/331.7 MB 37.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 232.8/331.7 MB 38.0 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 237.2/331.7 MB 37.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 243.8/331.7 MB 37.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 251.7/331.7 MB 37.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 259.8/331.7 MB 37.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 267.6/331.7 MB 37.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 276.0/331.7 MB 37.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 284.2/331.7 MB 37.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 292.8/331.7 MB 37.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 300.9/331.7 MB 37.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 306.2/331.7 MB 37.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 308.3/331.7 MB 36.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 316.9/331.7 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  325.1/331.7 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 331.7/331.7 MB 36.3 MB/s  0:00:09\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.2 MB/s  0:00:00\n",
      "Downloading ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl (210 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 8.7/12.9 MB 41.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 32.4 MB/s  0:00:00\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 33.7 MB/s  0:00:00\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 33.3 MB/s  0:00:00\n",
      "Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 30.5 MB/s  0:00:00\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 7.3/14.6 MB 37.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 35.4 MB/s  0:00:00\n",
      "Downloading google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: uritemplate, protobuf, numpy, httplib2, future, tensorboard, proto-plus, ml_dtypes, googleapis-common-protos, ffmpeg-python, keras, grpcio-status, google-auth-httplib2, google-api-core, tensorflow, google-api-python-client, tf-keras, google-ai-generativelanguage, google-generativeai\n",
      "\n",
      "   ----------------------------------------  0/19 [uritemplate]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   -- -------------------------------------  1/19 [protobuf]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ---- -----------------------------------  2/19 [numpy]\n",
      "   ------ ---------------------------------  3/19 [httplib2]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   -------- -------------------------------  4/19 [future]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ---------- -----------------------------  5/19 [tensorboard]\n",
      "   ------------ ---------------------------  6/19 [proto-plus]\n",
      "   ------------ ---------------------------  6/19 [proto-plus]\n",
      "   ------------ ---------------------------  6/19 [proto-plus]\n",
      "   ------------ ---------------------------  6/19 [proto-plus]\n",
      "   ------------ ---------------------------  6/19 [proto-plus]\n",
      "   -------------- -------------------------  7/19 [ml_dtypes]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  8/19 [googleapis-common-protos]\n",
      "   ------------------ ---------------------  9/19 [ffmpeg-python]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   --------------------- ------------------ 10/19 [keras]\n",
      "   ----------------------- ---------------- 11/19 [grpcio-status]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   --------------------------- ------------ 13/19 [google-api-core]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ----------------------------- ---------- 14/19 [tensorflow]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   ------------------------------- -------- 15/19 [google-api-python-client]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   --------------------------------- ------ 16/19 [tf-keras]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ---------------------------------- ---- 17/19 [google-ai-generativelanguage]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ------------------------------------- -- 18/19 [google-generativeai]\n",
      "   ---------------------------------------- 19/19 [google-generativeai]\n",
      "\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0 google-ai-generativelanguage-0.6.15 google-api-core-2.28.1 google-api-python-client-2.187.0 google-auth-httplib2-0.2.1 google-generativeai-0.8.5 googleapis-common-protos-1.72.0 grpcio-status-1.71.2 httplib2-0.31.0 keras-3.12.0 ml_dtypes-0.5.4 numpy-2.2.6 proto-plus-1.26.1 protobuf-5.29.5 tensorboard-2.20.0 tensorflow-2.20.0 tf-keras-2.20.1 uritemplate-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts futurize.exe and pasteurize.exe are installed in 'C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.14 requires jax, which is not installed.\n",
      "mediapipe 0.10.14 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.12.0 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.20.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# #   Mediapipe Whisper     \n",
    "# !pip install --user --upgrade --no-cache-dir \\\n",
    "#     streamlit \\\n",
    "#     google-generativeai \\\n",
    "#     deepface==0.0.95 \\\n",
    "#     tensorflow==2.20.0 \\\n",
    "#     keras==3.12.0 \\\n",
    "#     tf-keras \\\n",
    "#     opencv-python==4.12.0.88 \\\n",
    "#     ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29774c65-4649-4138-bd14-5c985a82a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (10.8.0)\n",
      "Requirement already satisfied: numba in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (0.62.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from openai-whisper) (2.2.6)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (0.12.0)\n",
      "Requirement already satisfied: torch in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (2.3.1+cu121)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from numba->openai-whisper) (0.45.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (4.15.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from torch->openai-whisper) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow 2.20.0 requires protobuf>=5.28.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.12.0 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.20.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (0.10.14)\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (3.10.7)\n",
      "Collecting numpy<2 (from mediapipe)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (4.12.0.88)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from jax->mediapipe) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\anaconda3\\envs\\emotion\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Downloading mediapipe-0.10.21-cp310-cp310-win_amd64.whl (51.0 MB)\n",
      "   ---------------------------------------- 0.0/51.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 6.0/51.0 MB 30.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 14.2/51.0 MB 34.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 20.4/51.0 MB 33.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 26.5/51.0 MB 31.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 32.0/51.0 MB 30.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 37.7/51.0 MB 30.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 41.4/51.0 MB 28.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 47.7/51.0 MB 28.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  50.9/51.0 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.0/51.0 MB 26.4 MB/s  0:00:01\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached jax-0.6.2-py3-none-any.whl (2.7 MB)\n",
      "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 6.6/46.2 MB 31.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.1/46.2 MB 30.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.7/46.2 MB 31.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 26.2/46.2 MB 30.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.8/46.2 MB 30.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.8/46.2 MB 31.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 31.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 29.4 MB/s  0:00:01\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 24.8 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece, protobuf, numpy, opencv-contrib-python, jax, mediapipe\n",
      "\n",
      "   ---------------------------------------- 0/6 [sentencepiece]\n",
      "  Attempting uninstall: protobuf\n",
      "   ---------------------------------------- 0/6 [sentencepiece]\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "   ---------------------------------------- 0/6 [sentencepiece]\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "   ---------------------------------------- 0/6 [sentencepiece]\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "   ---------------------------------------- 0/6 [sentencepiece]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "  Attempting uninstall: numpy\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "    Found existing installation: numpy 2.2.6\n",
      "   ------ --------------------------------- 1/6 [protobuf]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "    Uninstalling numpy-2.2.6:\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   ------------- -------------------------- 2/6 [numpy]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------- ------------------- 3/6 [opencv-contrib-python]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   -------------------------- ------------- 4/6 [jax]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   --------------------------------- ------ 5/6 [mediapipe]\n",
      "   ---------------------------------------- 6/6 [mediapipe]\n",
      "\n",
      "Successfully installed jax-0.6.2 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "# # 1. Whisper  (Triton    )\n",
    "# !pip install --user openai-whisper\n",
    "\n",
    "# # 2. Mediapipe  ( :  0.11+    Numpy 2.x  )\n",
    "# !pip install --user --upgrade mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ba887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg OS    .\n",
    "# - Windows: choco install ffmpeg     PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefed352-4dc3-4fea-b0f2-1bb12a1ebc8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from openai) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Using cached jiter-0.12.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.12.0-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   ---------------------------------------- 3/3 [openai]\n",
      "\n",
      "Successfully installed distro-1.9.0 jiter-0.12.0 openai-2.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9df87e5-7548-48ed-8b79-1aee0f264e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (0.5.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (1.15.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sounddevice) (2.0.0)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from CFFI>=1.0->sounddevice) (2.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0d60bc-c75d-4bb9-b4ef-da3eb9ad6597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " import \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    pipeline as hf_pipeline,\n",
    ")\n",
    "\n",
    "import whisper\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\" import \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfbe20",
   "metadata": {},
   "source": [
    "###    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e2b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: C:\\Users\\Owner\\Desktop\\SCI & \\SCI\n",
      "Wav2Vec2  : C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\wav2vec2_emotion_model\\70_accuracy_final_model\n",
      "MediaPipe  : C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\face_landmarker.task\n",
      "OPENAI_API_KEY  (GPT-4   )\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === 1)     ===\n",
    "BASE_PATH = Path(\".\").resolve()\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# === 2)  Wav2Vec2   ===\n",
    "WAV2VEC_MODEL_DIR = BASE_PATH / \"wav2vec2_emotion_model\" / \"70_accuracy_final_model\"\n",
    "print(\"Wav2Vec2  :\", WAV2VEC_MODEL_DIR)\n",
    "\n",
    "# === 3) MediaPipe face_landmarker   ===\n",
    "MEDIAPIPE_MODEL_PATH = BASE_PATH / \"face_landmarker.task\"\n",
    "print(\"MediaPipe  :\", MEDIAPIPE_MODEL_PATH)\n",
    "\n",
    "# === 4) Gpt API Key  ===\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1.    \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"OPENAI_API_KEY  (GPT-4   )\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY .    .\")\n",
    "\n",
    "os.environ.pop(\"SSL_CERT_FILE\", None)\n",
    "\n",
    "# 3.   \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# === 5)   ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf8978f-b70d-4e5b-9753-03f20f2ec930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFMPEG_BIN: C:\\Users\\Owner\\ffmpeg\\bin\\ffmpeg.exe\n",
      "ffmpeg.exe  : True\n",
      "ffmpeg  : ffmpeg version 2025-11-17-git-e94439e49b-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "#   \n",
    "FFMPEG_BIN = Path(r\"C:\\Users\\Owner\\ffmpeg\\bin\\ffmpeg.exe\")\n",
    "FFMPEG_DIR = FFMPEG_BIN.parent\n",
    "\n",
    "print(\"FFMPEG_BIN:\", FFMPEG_BIN)\n",
    "print(\"ffmpeg.exe  :\", FFMPEG_BIN.exists())\n",
    "\n",
    "os.environ[\"PATH\"] = str(FFMPEG_DIR) + os.pathsep + os.environ[\"PATH\"]\n",
    "\n",
    "try:\n",
    "    out = subprocess.check_output([str(FFMPEG_BIN), \"-version\"], stderr=subprocess.STDOUT)\n",
    "    first_line = out.decode(\"utf-8\", errors=\"ignore\").splitlines()[0]\n",
    "    print(\"ffmpeg  :\", first_line)\n",
    "except Exception as e:\n",
    "    print(\"ffmpeg  :\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad55ac-a88f-4164-a8fb-beae13ccbbef",
   "metadata": {},
   "source": [
    "###  llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0c1e6a-8470-449c-ae98-fbd5132382de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_json(\n",
    "    user_prompt: str,\n",
    "    system_prompt: str = \"\",\n",
    "    model: str = \"gpt-4o-mini\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    OpenAI GPT-4   ' JSON '  .\n",
    "    response_format={\"type\": \"json_object\"} .\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    content = resp.choices[0].message.content\n",
    "    return json.loads(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdedc70",
   "metadata": {},
   "source": [
    "###   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "455d555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import threading\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def record_realtime_multimodal(output_path=\"live_test.mp4\", duration=10):\n",
    "    \"\"\"\n",
    "    (Video) (Audio)     ( X,  )\n",
    "    \"\"\"\n",
    "    print(f\" [] {duration}    ...\")\n",
    "    \n",
    "    #   \n",
    "    temp_video = \"temp_video_only.mp4\"\n",
    "    temp_audio = \"temp_audio_only.wav\"\n",
    "    \n",
    "    # 1.    \n",
    "    fs = 44100  \n",
    "    print(\"   -    ...\")\n",
    "    \n",
    "    #    \n",
    "    audio_recording = []\n",
    "\n",
    "    def record_audio():\n",
    "        # duration  \n",
    "        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "        sd.wait()  #    \n",
    "        audio_recording.append(recording)\n",
    "\n",
    "    # 2.   \n",
    "    cap = cv2.VideoCapture(0) \n",
    "    if not cap.isOpened():\n",
    "        print(\"   .\")\n",
    "        return None\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = 30.0 #  FPS \n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))\n",
    "\n",
    "    # 3.   \n",
    "    print(\" [START]  !    .\")\n",
    "    \n",
    "    #   \n",
    "    audio_thread = threading.Thread(target=record_audio)\n",
    "    audio_thread.start()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #  \n",
    "    while (time.time() - start_time) < duration:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        #    \n",
    "        remain = duration - int(time.time() - start_time)\n",
    "        cv2.putText(frame, f\"REC: {remain}s\", (50, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Real-time Recording (Look at Camera)', frame)\n",
    "        out.write(frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 4.   \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    audio_thread.join()\n",
    "    \n",
    "    #   \n",
    "    if audio_recording:\n",
    "        wav.write(temp_audio, fs, audio_recording[0])\n",
    "    else:\n",
    "        print(\"  \")\n",
    "        return None\n",
    "\n",
    "    # 5. FFmpeg  ( + )\n",
    "    print(\" [MERGE]    ...\")\n",
    "    \n",
    "    #    \n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    # ffmpeg  \n",
    "    cmd = f'ffmpeg -y -i \"{temp_video}\" -i \"{temp_audio}\" -c:v copy -c:a aac \"{output_path}\" -loglevel quiet'\n",
    "    subprocess.call(cmd, shell=True)\n",
    "\n",
    "    #   \n",
    "    if os.path.exists(temp_video): os.remove(temp_video)\n",
    "    if os.path.exists(temp_audio): os.remove(temp_audio)\n",
    "\n",
    "    print(f\" !  : {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e465bb4-7a2e-4c40-831c-508ffaff0ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [] 10    ...\n",
      "   -    ...\n",
      " [START]  !    .\n",
      "- /  .\n",
      " [MERGE]    ...\n",
      " !  : my_live_test.mp4\n",
      "\n",
      "==============================\n",
      "     \n",
      "==============================\n",
      "============================\n",
      " run_full_pipeline \n",
      "============================\n",
      "\n",
      " ffmpeg   ...\n",
      "   -   : C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav\n",
      "\n",
      " Whisper STT  ...\n",
      "   - STT : So, what do you want? What do you want? What do you want? I want everybody.\n",
      "\n",
      " ( ) C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav  ...\n",
      "   -  : 16000, shape: (160125,)\n",
      "   -  : {'happy': 0.023309867829084396, 'angry': 0.9431574940681458, 'fearful': 0.006897819694131613, 'sad': 0.004751851316541433, 'surprised': 0.014356349594891071, 'disgust': 0.0075265769846737385}\n",
      "\n",
      " ( ) C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav  ...\n",
      "   -  : sr=16000, len=160125)\n",
      "   -   :  (RMS) 0.006 .   (ZCR) 0.114,  /  .     2884.0Hz,    .   (F0)  153.8Hz.\n",
      "\n",
      " ( ) GPT-4    ...\n",
      "\n",
      " ( ) GYAN  GPT-4  ...\n",
      "\n",
      " ( ) DeepFace   ...\n",
      "   -  : my_live_test.mp4\n",
      "   -   : 273, FPS: 30.00\n",
      "   - DeepFace   : 19\n",
      "   -  : {'sad': 5, 'happy': 3, 'angry': 3, 'neutral': 6, 'fear': 2}\n",
      "\n",
      " ( ) MediaPipe blendshapes  ...\n",
      "\n",
      " ( ) LLM    ...\n",
      "\n",
      " ()    ...\n",
      "\n",
      "==============================\n",
      "[  ]\n",
      "==============================\n",
      "1. Final Emotion Conclusion: Angry\n",
      "2. Confidence Score: 80%\n",
      "3. Key Evidence Analysis: The audio analysis and the text analysis both indicate that the individual is angry. The text analysis, in particular, shows a high level of anger at 0.8, which is further supported by the audio analysis, which shows an anger score of 0.943. The video analysis, however, suggests that the individual's dominant emotion is neutral. This conflict can be resolved by prioritizing text and video evidence (micro-expressions), as per the constraints. In this case, the text evidence has a stronger indication of anger, which aligns with the audio analysis. \n",
      "4. Mental Health Warning: Yes. The high level of anger indicated in the audio and text analysis, combined with the sense of urgency or insistence suggested by the repetition in the text transcript, might suggest a potential issue with aggression or impulse control. It would be advisable for this individual to seek a professional mental health consultation.\n",
      "\n",
      "[AI  ]\n",
      "Hey there, I sense that you're going through a tough time right now and I want you to know it's okay to feel angry. Remember, all our feelings are valid and important. However, if you're consistently feeling this way, it might be helpful to chat with a mental health professional. They can provide you with the support and coping strategies to navigate these feelings. Remember, reaching out is a big step towards finding balance and you're not alone in this journey.\n",
      "\n",
      "[ ]\n",
      "-  : angry\n",
      "-  : anger\n",
      "-  : {\"deepface_counts\": {\"dominant_emotion\": \"neutral\", \"emotion_counts\": {\"sad\": 5, \"happy\": 3, \"angry\": 3, \"neutral\": 6, \"fear\": 2}, \"emotion_ratio\": {\"sad\": 0.2631578947368421, \"happy\": 0.1578947368421 ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1.    (10 )\n",
    "live_file_path = record_realtime_multimodal(output_path=\"my_live_test.mp4\", duration=10)\n",
    "\n",
    "# 2.      \n",
    "if live_file_path:\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"     \")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    live_result = run_full_pipeline(live_file_path)\n",
    "    \n",
    "    # 3.  \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"[  ]\")\n",
    "    print(\"=\"*30)\n",
    "    print(live_result[\"final_judgment\"])\n",
    "    \n",
    "    print(\"\\n[AI  ]\")\n",
    "    print(live_result[\"chat_and_summary\"][\"message\"])\n",
    "    \n",
    "    # 4.    \n",
    "    print(\"\\n[ ]\")\n",
    "    print(f\"-  : {live_result['audio_emotion']['top_label']}\")\n",
    "    print(f\"-  : {live_result['text_emotion']['hypothesis']['label']}\")\n",
    "    #   \n",
    "    face_str = json.dumps(live_result['face_summary'], ensure_ascii=False)\n",
    "    print(f\"-  : {face_str[:200]} ...\")\n",
    "\n",
    "else:\n",
    "    print(\"     .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c3e0",
   "metadata": {},
   "source": [
    "###   ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aee471f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Wav2Vec2  : C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\70_accuracy_final_model\n",
      "    : True\n",
      "\n",
      "Base checkpoint feature_extractor  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wav2Vec2   ...\n",
      "\n",
      "/  \n",
      "     : {0: 'happy', 1: 'angry', 2: 'fearful', 3: 'sad', 4: 'surprised', 5: 'disgust'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# ---------------------------\n",
    "# 1.  \n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# 2.  Wav2Vec2  \n",
    "# ---------------------------\n",
    "WAV2VEC_MODEL_DIR = Path(\n",
    "    r\"C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\70_accuracy_final_model\"\n",
    ")\n",
    "\n",
    "print(\"Wav2Vec2  :\", WAV2VEC_MODEL_DIR)\n",
    "print(\"    :\", WAV2VEC_MODEL_DIR.exists())\n",
    "\n",
    "if not WAV2VEC_MODEL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"     : {WAV2VEC_MODEL_DIR}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. feature_extractor base   \n",
    "# ---------------------------\n",
    "BASE_WAV2VEC_CHECKPOINT = \"facebook/wav2vec2-base-960h\"\n",
    "print(\"\\nBase checkpoint feature_extractor  ...\")\n",
    "speech_feature_extractor = AutoFeatureExtractor.from_pretrained(BASE_WAV2VEC_CHECKPOINT)\n",
    "\n",
    "# ---------------------------\n",
    "# 4.      \n",
    "# ---------------------------\n",
    "print(\" Wav2Vec2   ...\")\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    str(WAV2VEC_MODEL_DIR)\n",
    ").to(device)\n",
    "\n",
    "id2label = speech_model.config.id2label\n",
    "print(\"\\n/  \")\n",
    "print(\"     :\", id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb2b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " analyze_speech_hypothesis / analyze_speech_evidence  \n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# (1)  : Wav2Vec2  \n",
    "# -----------------------------\n",
    "def analyze_speech_hypothesis(\n",
    "    audio_path: str,\n",
    "    model,\n",
    "    feature_extractor,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "     Wav2Vec2 '(hypothesis)'   \n",
    "    \"\"\"\n",
    "    print(f\"\\n ( ) {audio_path}  ...\")\n",
    "\n",
    "    # 1)   (16kHz)\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    print(f\"   -  : {sr}, shape: {y.shape}\")\n",
    "\n",
    "    # 2) Wav2Vec2   \n",
    "    inputs = feature_extractor(\n",
    "        y,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "\n",
    "    # 3)  \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    # 4)  \n",
    "    id2label = model.config.id2label\n",
    "    prob_dict = {\n",
    "        id2label[i]: float(p) for i, p in enumerate(probs)\n",
    "    }\n",
    "    top_idx = int(np.argmax(probs))\n",
    "    top_label = id2label[top_idx]\n",
    "    top_score = float(probs[top_idx])\n",
    "\n",
    "    print(f\"   -  : {prob_dict}\")\n",
    "\n",
    "    return {\n",
    "        \"probs\": prob_dict,\n",
    "        \"top_label\": top_label,\n",
    "        \"top_score\": top_score,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# (2)  : librosa   feature\n",
    "# -----------------------------\n",
    "def analyze_speech_evidence(\n",
    "    audio_path: str,\n",
    "    model=None,\n",
    "    feature_extractor=None,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "     '(Evidence)' :\n",
    "    - Wav2Vec2    librosa   \n",
    "    - run_full_pipeline model, feature_extractor, device   \n",
    "           \n",
    "    \"\"\"\n",
    "    print(f\"\\n ( ) {audio_path}  ...\")\n",
    "\n",
    "    # 1)  \n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    print(f\"   -  : sr={sr}, len={len(y)})\")\n",
    "\n",
    "    # 2)    \n",
    "    rms = float(librosa.feature.rms(y=y).mean())\n",
    "    zcr = float(librosa.feature.zero_crossing_rate(y).mean())\n",
    "    centroid = float(librosa.feature.spectral_centroid(y=y, sr=sr).mean())\n",
    "\n",
    "    # 3) F0 \n",
    "    try:\n",
    "        f0 = librosa.yin(y, fmin=50, fmax=500, sr=sr)\n",
    "        f0_mean = float(np.nanmean(f0))\n",
    "    except Exception as e:\n",
    "        print(\"   - F0   , NaN :\", e)\n",
    "        f0_mean = float(\"nan\")\n",
    "\n",
    "    evidence_features = {\n",
    "        \"rms_mean\": rms,\n",
    "        \"zcr_mean\": zcr,\n",
    "        \"spectral_centroid_mean\": centroid,\n",
    "        \"f0_mean\": f0_mean,\n",
    "    }\n",
    "\n",
    "    # 4) LLM   \n",
    "    desc_parts = []\n",
    "    desc_parts.append(f\" (RMS) {rms:.3f} .\")\n",
    "    desc_parts.append(f\"  (ZCR) {zcr:.3f},  /  .\")\n",
    "    desc_parts.append(f\"    {centroid:.1f}Hz,    .\")\n",
    "    if not np.isnan(f0_mean):\n",
    "        desc_parts.append(f\"  (F0)  {f0_mean:.1f}Hz.\")\n",
    "\n",
    "    evidence_text = \" \".join(desc_parts)\n",
    "\n",
    "    print(\"   -   :\", evidence_text)\n",
    "\n",
    "    return {\n",
    "        \"features\": evidence_features,\n",
    "        \"description\": evidence_text,\n",
    "    }\n",
    "\n",
    "print(\"\\n analyze_speech_hypothesis / analyze_speech_evidence  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf2168-7a4a-41d5-b347-70c537374109",
   "metadata": {},
   "source": [
    "### STT &   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad5fc6-6509-4e6f-8729-813d7b8337a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96bd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI   \n",
      "\n",
      "Whisper    (base)...\n",
      "Whisper  \n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3. STT(Whisper) +    (GPT-4 )\n",
    "# =========================================\n",
    "\n",
    "import json\n",
    "import whisper\n",
    "import subprocess\n",
    "\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"OpenAI   \")\n",
    "\n",
    "print(\"\\nWhisper    (base)...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "print(\"Whisper  \")\n",
    "\n",
    "\n",
    "# --- 3-1. ffmpeg     ---\n",
    "def extract_audio_from_video(video_path: str, out_wav_path: str, sr: int = 16000):\n",
    "    \"\"\"\n",
    "    ffmpeg  (wav) \n",
    "    \"\"\"\n",
    "    print(\"\\n ffmpeg   ...\")\n",
    "    cmd = [\n",
    "        str(FFMPEG_BIN),  \n",
    "        \"-y\",\n",
    "        \"-i\", video_path,\n",
    "        \"-vn\",\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ar\", str(sr),\n",
    "        \"-ac\", \"1\",\n",
    "        out_wav_path,\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(\"   -   :\", out_wav_path)\n",
    "    except Exception as e:\n",
    "        print(\"   - ffmpeg :\", e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# --- 3-2. Whisper STT ---\n",
    "def transcribe_audio_with_whisper(audio_path: str) -> str:\n",
    "    \"\"\"Whisper  STT \"\"\"\n",
    "    print(\"\\n Whisper STT  ...\")\n",
    "    try:\n",
    "        result = whisper_model.transcribe(audio_path)\n",
    "        text = result.get(\"text\", \"\").strip()\n",
    "        print(\"   - STT :\", text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"   - Whisper :\", e)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# --- 3-3. GPT-4 JSON   ---\n",
    "import json\n",
    "\n",
    "def call_gpt4_json(prompt: str, model: str = \"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    GPT-4   JSON    \n",
    "    - model gpt-4.1, gpt-4.1-mini, gpt-4o   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(\"   - GPT-4 /JSON  :\", e)\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# --- 3-4.   '' ( ) ---\n",
    "def analyze_text_hypothesis(text: str):\n",
    "    \"\"\"\n",
    "        (GPT-4)\n",
    "    - label:   \n",
    "    - scores:      (0~1,  1  )\n",
    "    \"\"\"\n",
    "    print(\"\\n ( ) GPT-4    ...\")\n",
    "\n",
    "    if not text:\n",
    "        return {\"error\": \"empty_text\"}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an emotion classifier for English text.\n",
    "\n",
    "[Text]:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "\n",
    "[Task]\n",
    "1. Classify the PRIMARY emotion in the text.\n",
    "2. Use ONLY the following labels:\n",
    "   [\"happiness\", \"anger\", \"fear\", \"sadness\", \"surprise\", \"disgust\", \"neutral\"]\n",
    "3. Also provide a relative score (0~1) for each label, reflecting how likely it is.\n",
    "\n",
    "[Output Format - JSON only]:\n",
    "{{\n",
    "  \"label\": \"<one of the 7 labels>\",\n",
    "  \"scores\": {{\n",
    "     \"happiness\": 0.0,\n",
    "     \"anger\": 0.0,\n",
    "     \"fear\": 0.0,\n",
    "     \"sadness\": 0.0,\n",
    "     \"surprise\": 0.0,\n",
    "     \"disgust\": 0.0,\n",
    "     \"neutral\": 0.0\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "    return call_gpt4_json(prompt)\n",
    "\n",
    "\n",
    "# --- 3-5.   '' (GYAN   ) ---\n",
    "def analyze_text_evidence(text: str):\n",
    "    \"\"\"GYAN     (GPT-4)\"\"\"\n",
    "    print(\"\\n ( ) GYAN  GPT-4  ...\")\n",
    "\n",
    "    if not text:\n",
    "        return {\"error\": \"empty_text\"}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a 'GYAN-style' deep analyst. Analyze the [Text] and respond ONLY in JSON format.\n",
    "\n",
    "[Text]: \"{text}\"\n",
    "\n",
    "[Analysis Items]:\n",
    "1. \"subject\": Who is the speaker? (e.g., \"Self\")\n",
    "2. \"action_event\": What is the main topic?\n",
    "3. \"observed_response\": What is the speaker's observable response?\n",
    "4. \"context_description\": What is the hidden context or intent?\n",
    "5. \"fact_claim\": Separate facts from claims.\n",
    "6. \"gyan_judged_emotion\": What is the TRUE underlying emotion? Choose ONLY ONE: [\"happiness\", \"anger\", \"fear\", \"sadness\", \"surprise\", \"disgust\", \"neutral\"].\n",
    "\n",
    "[Output Format]: Json\n",
    "\"\"\"\n",
    "    return call_gpt4_json(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c03f4-cf35-4746-a9a2-cb29d566306c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab28201-f0d9-4576-a923-c482c47b9098",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402ddf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 5.  \n",
    "# =========================================\n",
    "\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python as mp_python\n",
    "from mediapipe.tasks.python import vision as mp_vision\n",
    "\n",
    "# -----------------------------\n",
    "# (1)  : DeepFace  \n",
    "# -----------------------------\n",
    "def analyze_video_hypothesis(\n",
    "    video_path: str,\n",
    "    frame_step: int = 15,   #     \n",
    "):\n",
    "    \"\"\"\n",
    "        ,\n",
    "    DeepFace    '(hypothesis)' .\n",
    "\n",
    "    :\n",
    "      {\n",
    "        \"dominant_emotion\": \"happy\",\n",
    "        \"emotion_counts\": {...},   #   \n",
    "        \"emotion_ratio\": {...},    #   (0~1)\n",
    "        \"summary\": \"  \"\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n ( ) DeepFace   ...\")\n",
    "    print(f\"   -  : {video_path}\")\n",
    "\n",
    "    video_path = str(video_path)\n",
    "    if not os.path.exists(video_path):\n",
    "        msg = f\"    : {video_path}\"\n",
    "        print(\"   - :\", msg)\n",
    "        return {\"error\": msg}\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        msg = f\"    : {video_path}\"\n",
    "        print(\"   - :\", msg)\n",
    "        return {\"error\": msg}\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n",
    "    print(f\"   -   : {total_frames}, FPS: {fps:.2f}\")\n",
    "\n",
    "    emotion_counts = Counter()\n",
    "    analyzed_frames = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frame_idx = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "            # frame_step  \n",
    "            if frame_idx % frame_step != 0:\n",
    "                ret = cap.grab()\n",
    "                if not ret:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            #     !!!( !)_\n",
    "            h, w = frame.shape[:2]\n",
    "            if max(h, w) > 720:\n",
    "                scale = 720 / max(h, w)\n",
    "                frame = cv2.resize(frame, (int(w * scale), int(h * scale)))\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            try:\n",
    "                # prog_bar \n",
    "                result = DeepFace.analyze(\n",
    "                    img_path=frame,\n",
    "                    actions=['emotion'],\n",
    "                    enforce_detection=False,\n",
    "                    detector_backend='opencv',\n",
    "                    silent=True  \n",
    "                )\n",
    "               \n",
    "                if isinstance(result, list):\n",
    "                    result = result[0]\n",
    "\n",
    "                dom = result.get(\"dominant_emotion\", None)\n",
    "                emo_dict = result.get(\"emotion\", {})\n",
    "\n",
    "                if dom is not None:\n",
    "                    emotion_counts[dom] += 1\n",
    "                    analyzed_frames += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   -  {frame_idx} DeepFace   : {e}\")\n",
    "                continue\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    print(f\"   - DeepFace   : {analyzed_frames}\")\n",
    "    print(f\"   -  : {dict(emotion_counts)}\")\n",
    "\n",
    "    if analyzed_frames == 0:\n",
    "        return {\n",
    "            \"error\": \"no_valid_face\",\n",
    "            \"summary\": \"        .\"\n",
    "        }\n",
    "\n",
    "    #    \n",
    "    dominant_emotion, top_count = emotion_counts.most_common(1)[0]\n",
    "\n",
    "    #  \n",
    "    emotion_ratio = {\n",
    "        emo: count / analyzed_frames\n",
    "        for emo, count in emotion_counts.items()\n",
    "    }\n",
    "\n",
    "    summary = (\n",
    "        f\"   {frame_step}   \"\n",
    "        f\"{analyzed_frames}     , \"\n",
    "        f\"'{dominant_emotion}'    . \"\n",
    "        f\"   {emotion_ratio} .\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"dominant_emotion\": dominant_emotion,\n",
    "        \"emotion_counts\": dict(emotion_counts),\n",
    "        \"emotion_ratio\": emotion_ratio,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# (2)  : MediaPipe FaceLandmarker blendshapes\n",
    "# -----------------------------\n",
    "\n",
    "INTERESTING_BLENDSHAPES = [\n",
    "    \"browDownLeft\", \"browDownRight\",\n",
    "    \"mouthFrownLeft\", \"mouthFrownRight\",\n",
    "    \"mouthSmileLeft\", \"mouthSmileRight\",\n",
    "    \"jawOpen\",\n",
    "    \"eyeWideLeft\", \"eyeWideRight\",\n",
    "    \"noseSneerLeft\", \"noseSneerRight\",\n",
    "    \"eyeSquintLeft\", \"eyeSquintRight\",\n",
    "]\n",
    "\n",
    "import mediapipe as mp #     \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def analyze_video_evidence(video_path, model_path):\n",
    "    print(f\"\\n ( ) MediaPipe blendshapes  ...\")\n",
    "    \n",
    "    # [1]     !\n",
    "    import mediapipe as mp\n",
    "    import os\n",
    "    \n",
    "    BaseOptions = mp.tasks.BaseOptions\n",
    "    FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "    FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "    # [2]    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\":     -> {model_path}\")\n",
    "        return {}\n",
    "\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model_buffer = f.read()\n",
    "\n",
    "    # [3]  \n",
    "    base_options = BaseOptions(model_asset_buffer=model_buffer)\n",
    "    \n",
    "    options = FaceLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        running_mode=VisionRunningMode.VIDEO,\n",
    "        output_face_blendshapes=True,\n",
    "    )\n",
    "    \n",
    "    #  Blendshapes \n",
    "    INTERESTING_BLENDSHAPES = {\n",
    "        \"browDownLeft\", \"browDownRight\",\n",
    "        \"browInnerUp\",\n",
    "        \"browOuterUpLeft\", \"browOuterUpRight\",\n",
    "        \"eyeBlinkLeft\", \"eyeBlinkRight\",\n",
    "        \"mouthSmileLeft\", \"mouthSmileRight\",\n",
    "        \"mouthFrownLeft\", \"mouthFrownRight\"\n",
    "    }\n",
    "    \n",
    "    scores = {name: [] for name in INTERESTING_BLENDSHAPES}\n",
    "\n",
    "    # [4]  \n",
    "    with FaceLandmarker.create_from_options(options) as landmarker:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"  : {video_path}\")\n",
    "            return {}\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_idx = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # MediaPipe  \n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            #  (ms)\n",
    "            timestamp_ms = int((frame_idx / fps) * 1000)\n",
    "            \n",
    "            #  \n",
    "            result = landmarker.detect_for_video(mp_image, timestamp_ms)\n",
    "            \n",
    "            if result.face_blendshapes:\n",
    "                #     \n",
    "                shapes = result.face_blendshapes[0] \n",
    "                \n",
    "                # shapes Category    \n",
    "                temp_map = {cat.category_name: cat.score for cat in shapes}\n",
    "                \n",
    "                for name in INTERESTING_BLENDSHAPES:\n",
    "                    if name in temp_map:\n",
    "                        scores[name].append(temp_map[name])\n",
    "            \n",
    "            frame_idx += 1\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "    # [5]   ()\n",
    "    final_stats = {}\n",
    "    for name, val_list in scores.items():\n",
    "        if val_list:\n",
    "            final_stats[name] = float(np.mean(val_list))\n",
    "        else:\n",
    "            final_stats[name] = 0.0\n",
    "            \n",
    "    return final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad8318-9243-42bc-8a24-7559ef141ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd84db5-0d5a-4cbc-8f4e-278de6810f1f",
   "metadata": {},
   "source": [
    "###   llm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "554e20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# =========================================\n",
    "# 1. LLM    \n",
    "# =========================================\n",
    "def run_llm_final_judgment(audio_emotion, audio_evidence, text_emotion, video_emotion, video_evidence, transcript):\n",
    "    print(\"\\n ( )      ...\")\n",
    "\n",
    "    #  :      \n",
    "    system_prompt = \"\"\"\n",
    "         '  (Master Multimodal Analyst)'.\n",
    "    , ,    '(Hypothesis)' '(Evidence)'  ,  ' (Real Emotion)'  .\n",
    "\n",
    "    [ ]\n",
    "    1. ** (Conflict Detection):**  ''  '',  ''   '' ,  \"(Masking)\"  \" \" .\n",
    "    2. ** (Evidence First):**  ()  (:  ,  ,  )  .\n",
    "    3. **  (Mental Health Assessment):**       , , ,      .\n",
    "\n",
    "    [ ]\n",
    "      **JSON **  . (Markdown    raw JSON )\n",
    "    {\n",
    "        \"rationale\": \"     . \\n1. ** vs  :** ... \\n2. **  :** ... \\n3. ** :** ...\",\n",
    "        \"final_emotion\": \"sadness\",\n",
    "        \"mental_health_warning\": \"          (: '.         .')\"\n",
    "    }\n",
    "    \n",
    "    * final_emotion : [\"happiness\", \"anger\", \"fear\", \"sadness\", \"surprise\", \"disgust\", \"neutral\"]   1\n",
    "    \"\"\"\n",
    "\n",
    "    #  :     !!\n",
    "    user_prompt = f\"\"\"\n",
    "    [ ]\n",
    "    \n",
    "    1. (Audio)\n",
    "    - (AI ): {audio_emotion}\n",
    "    - (): {audio_evidence}\n",
    "    \n",
    "    2. (Text)\n",
    "    -  : \"{transcript}\"\n",
    "    -  : {text_emotion}\n",
    "    \n",
    "    3. (Video)\n",
    "    - (DeepFace): {video_emotion}\n",
    "    - (MediaPipe Blendshapes): {video_evidence}\n",
    "\n",
    "       \"rationale\", \"final_emotion\", \"mental_health_warning\" JSON .\n",
    "     'happy' 'sad'  ,         .\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.3  #     \n",
    "        )\n",
    "        \n",
    "        #   \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        if content.startswith(\"```\"):\n",
    "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "        return content  # JSON   \n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"rationale\": f\" : {str(e)}\", \n",
    "            \"final_emotion\": \"error\", \n",
    "            \"mental_health_warning\": \" \"\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2.    \n",
    "# =========================================\n",
    "def run_empathy_chat_and_summary(final_judgment):\n",
    "    print(\"\\n ()    ...\")\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an empathetic AI counselor. \n",
    "    Based on the expert's analysis provided, write a warm, short, and comforting message to the user (in Korean).\n",
    "    Do not mention technical terms like 'DeepFace' or 'ZCR'. Speak naturally like a caring friend.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Expert Analysis:\\n{final_judgment}\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        msg = response.choices[0].message.content\n",
    "        return {\"message\": msg, \"original_analysis\": final_judgment}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"message\": \"     .\", \"original_analysis\": str(e)}\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3.  \n",
    "# =========================================\n",
    "def run_full_pipeline(video_path: str):\n",
    "    \"\"\"\n",
    "    1) ffmpeg   \n",
    "    2) Whisper STT\n",
    "    3)   (/) - Wav2Vec2\n",
    "    4)   \n",
    "    5) / \n",
    "    6) GPT-4o  (final_judgment) \n",
    "    \"\"\"\n",
    "    print(\"============================\")\n",
    "    print(\" run_full_pipeline \")\n",
    "    print(\"============================\")\n",
    "\n",
    "    # 0)    \n",
    "    temp_audio_path = BASE_PATH / \"temp_extracted_audio.wav\"\n",
    "\n",
    "    # 1) ffmpeg  \n",
    "    extract_audio_from_video(str(video_path), str(temp_audio_path))\n",
    "\n",
    "    # 2) Whisper STT\n",
    "    transcribed_text = transcribe_audio_with_whisper(str(temp_audio_path))\n",
    "\n",
    "    # 3)  : \n",
    "    speech_hyp = analyze_speech_hypothesis(\n",
    "        audio_path=str(temp_audio_path),\n",
    "        model=speech_model,\n",
    "        feature_extractor=speech_feature_extractor,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 4)  : \n",
    "    speech_evi = analyze_speech_evidence(\n",
    "        audio_path=str(temp_audio_path),\n",
    "        model=speech_model,\n",
    "        feature_extractor=speech_feature_extractor,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 5)   \n",
    "    text_hyp = analyze_text_hypothesis(transcribed_text)\n",
    "    text_evi = analyze_text_evidence(transcribed_text)\n",
    "    text_emotion = {\n",
    "        \"hypothesis\": text_hyp,\n",
    "        \"evidence\": text_evi,\n",
    "    }\n",
    "\n",
    "    # 6)   +  \n",
    "    video_hyp = analyze_video_hypothesis(str(video_path))\n",
    "    video_evi = analyze_video_evidence(str(video_path), str(MEDIAPIPE_MODEL_PATH))\n",
    "\n",
    "    # 7) LLM  \n",
    "    final_judgment = run_llm_final_judgment(\n",
    "        audio_emotion=speech_hyp,\n",
    "        audio_evidence=speech_evi,\n",
    "        text_emotion=text_emotion,\n",
    "        video_emotion=video_hyp,\n",
    "        video_evidence=video_evi,\n",
    "        transcript=transcribed_text,\n",
    "    )\n",
    "\n",
    "    # 8)   + \n",
    "    chat_and_summary = run_empathy_chat_and_summary(final_judgment)\n",
    "\n",
    "    face_summary = {\n",
    "        \"deepface_counts\": video_hyp,\n",
    "        \"mediapipe_stats\": video_evi\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"audio_emotion\": speech_hyp,\n",
    "        \"audio_evidence\": speech_evi,\n",
    "        \"text_emotion\": text_emotion,\n",
    "        \"face_summary\": face_summary,\n",
    "        \"final_judgment\": final_judgment,\n",
    "        \"chat_and_summary\": chat_and_summary,\n",
    "        \"transcript\": transcribed_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2ffcd-22a6-4d72-874a-9833438e2f3c",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a3c4ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [] 10    ...\n",
      "   -    ...\n",
      " [START]  !    .\n",
      " [MERGE]    ...\n",
      " !  : live_test.mp4\n",
      "\n",
      "========================================\n",
      "    ...\n",
      "========================================\n",
      "============================\n",
      " run_full_pipeline \n",
      "============================\n",
      "\n",
      " ffmpeg   ...\n",
      "   -   : C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav\n",
      "\n",
      " Whisper STT  ...\n",
      "   - STT : Hi everybody. Good morning.\n",
      "\n",
      " ( ) C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav  ...\n",
      "   -  : 16000, shape: (160125,)\n",
      "   -  : {'happy': 0.0342191718518734, 'angry': 0.9076000452041626, 'fearful': 0.011955803260207176, 'sad': 0.006116040516644716, 'surprised': 0.026479186490178108, 'disgust': 0.013629790395498276}\n",
      "\n",
      " ( ) C:\\Users\\Owner\\Desktop\\SCI & \\SCI\\temp_extracted_audio.wav  ...\n",
      "   -  : sr=16000, len=160125)\n",
      "   -   :  (RMS) 0.011 .   (ZCR) 0.123,  /  .     3003.3Hz,    .   (F0)  171.3Hz.\n",
      "\n",
      " ( ) GPT-4    ...\n",
      "\n",
      " ( ) GYAN  GPT-4  ...\n",
      "\n",
      " ( ) DeepFace   ...\n",
      "   -  : live_test.mp4\n",
      "   -   : 274, FPS: 30.00\n",
      "   - DeepFace   : 19\n",
      "   -  : {'sad': 6, 'neutral': 10, 'happy': 3}\n",
      "\n",
      " ( ) MediaPipe blendshapes  ...\n",
      "\n",
      " ( )      ...\n",
      "\n",
      " ()    ...\n",
      "\n",
      "=====    =====\n",
      "Top Label: angry\n",
      "{'probs': {'happy': 0.0342191718518734, 'angry': 0.9076000452041626, 'fearful': 0.011955803260207176, 'sad': 0.006116040516644716, 'surprised': 0.026479186490178108, 'disgust': 0.013629790395498276}, 'top_label': 'angry', 'top_score': 0.9076000452041626}\n",
      "\n",
      "=====    =====\n",
      "{'hypothesis': {'label': 'neutral', 'scores': {'happiness': 0.2, 'anger': 0.0, 'fear': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'disgust': 0.0, 'neutral': 0.8}}, 'evidence': {'subject': 'Self', 'action_event': 'Greeting the audience', 'observed_response': \"The speaker says 'Hi everybody. Good morning.'\", 'context_description': 'The speaker intends to initiate communication and establish a friendly, welcoming atmosphere at the start of an interaction or event.', 'fact_claim': {'fact': \"The speaker greeted the audience with 'Hi everybody. Good morning.'\", 'claim': 'The greeting implies a positive and polite social interaction.'}, 'gyan_judged_emotion': 'happiness'}}\n",
      "\n",
      "===== /   =====\n",
      "{'deepface_counts': {'dominant_emotion': 'neutral', 'emotion_counts': {'sad': 6, 'neutral': 10, 'happy': 3}, 'emotion_ratio': {'sad': 0.3157894736842105, 'neutral': 0.5263157894736842, 'happy': 0.15789473684210525}, 'summary': \"   15   19     , 'neutral'    .    {'sad': 0.3157894736842105, 'neutral': 0.5263157894736842, 'happy': 0.15789473684210525} .\"}, 'mediapipe_stats': {'eyeBlinkLeft': 0.255047212579172, 'browDownRight': 0.00303745781153 ...\n",
      "\n",
      "===== [  ] =====\n",
      "{\n",
      "    \"rationale\": \"1. ** vs  :**     ''  ,       '', '' ''   .     ,       . 2. **  :**    'Hi everybody. Good morning.',     .          . 3. ** :**   ''    ''  ''    .           .       ,  ''  ,  ''    .\",\n",
      "    \"final_emotion\": \"neutral\",\n",
      "    \"mental_health_warning\": \".      ,      .         .\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1.   \n",
    "saved_file_path = record_realtime_multimodal(output_path=\"live_test.mp4\", duration=10)\n",
    "\n",
    "# 2.     \n",
    "if saved_file_path:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"    ...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    #   \n",
    "    result = run_full_pipeline(saved_file_path)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3.   \n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n=====    =====\")\n",
    "\n",
    "    print(f\"Top Label: {result['audio_emotion']['top_label']}\")\n",
    "    print(result[\"audio_emotion\"])\n",
    "\n",
    "    print(\"\\n=====    =====\")\n",
    "    print(result[\"text_emotion\"])\n",
    "\n",
    "    print(\"\\n===== /   =====\")\n",
    "    print(str(result[\"face_summary\"])[:500] + \" ...\")\n",
    "    \n",
    "else:\n",
    "    print(\" .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb8813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [  ] =====\n",
      "{\n",
      "    \"rationale\": \"1. ** vs  :**     ''  ,       '', '' ''   .     ,       . 2. **  :**    'Hi everybody. Good morning.',     .          . 3. ** :**   ''    ''  ''    .           .       ,  ''  ,  ''    .\",\n",
      "    \"final_emotion\": \"neutral\",\n",
      "    \"mental_health_warning\": \".      ,      .         .\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\n===== [  ] =====\")\n",
    "    try:\n",
    "        parsed_json = json.loads(result[\"final_judgment\"])\n",
    "        print(json.dumps(parsed_json, indent=4, ensure_ascii=False))\n",
    "    except:\n",
    "        print(result[\"final_judgment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554c9ab-aae5-4731-b94d-7e50c3a6f4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9b640a-e273-414b-9942-c7869ea92af5",
   "metadata": {},
   "source": [
    "###   ,,,!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6573ca7e-a135-42c9-a3d2-72ea07c649a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    !\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements_final.txt\n",
    "print(\"    !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd8244-d58d-4d1c-8164-693efe119d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e11012-56a2-442e-b813-2c51daea1eda",
   "metadata": {},
   "source": [
    "### RAG  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d71f8047-a2dd-4bb4-8dc0-8eb1af2b281d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.0-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pypdf) (4.15.0)\n",
      "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
      "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.44-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.46-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.4)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.11.4-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.25.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.4.2)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.2.4-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\owner\\anaconda3\\envs\\emotion_ai\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\owner\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 2.1/2.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 9.1 MB/s  0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 7.1 MB/s  0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.46-py3-none-any.whl (411 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.2 MB/s  0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading faiss_cpu-1.13.0-cp310-cp310-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.8/18.7 MB 10.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.7/18.7 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.0/18.7 MB 9.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.1/18.7 MB 9.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 10.2/18.7 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 12.3/18.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 14.4/18.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.5/18.7 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.6/18.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 9.6 MB/s  0:00:01\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading greenlet-3.2.4-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading orjson-3.11.4-cp310-cp310-win_amd64.whl (131 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp310-cp310-win_amd64.whl (506 kB)\n",
      "Installing collected packages: zstandard, python-dotenv, pypdf, orjson, mypy-extensions, marshmallow, jsonpatch, httpx-sse, greenlet, faiss-cpu, async-timeout, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic-settings, dataclasses-json, langsmith, sentence-transformers, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
      "\n",
      "   - --------------------------------------  1/22 [python-dotenv]\n",
      "   --- ------------------------------------  2/22 [pypdf]\n",
      "   --- ------------------------------------  2/22 [pypdf]\n",
      "   --- ------------------------------------  2/22 [pypdf]\n",
      "   ------- --------------------------------  4/22 [mypy-extensions]\n",
      "   ---------- -----------------------------  6/22 [jsonpatch]\n",
      "   -------------- -------------------------  8/22 [greenlet]\n",
      "   -------------- -------------------------  8/22 [greenlet]\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "  Attempting uninstall: async-timeout\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "   ---------------- -----------------------  9/22 [faiss-cpu]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   --------------------- ------------------ 12/22 [SQLAlchemy]\n",
      "   ----------------------- ---------------- 13/22 [requests-toolbelt]\n",
      "   ------------------------- -------------- 14/22 [pydantic-settings]\n",
      "   ----------------------------- ---------- 16/22 [langsmith]\n",
      "   ----------------------------- ---------- 16/22 [langsmith]\n",
      "   ------------------------------ --------- 17/22 [sentence-transformers]\n",
      "   ------------------------------ --------- 17/22 [sentence-transformers]\n",
      "   ------------------------------ --------- 17/22 [sentence-transformers]\n",
      "   -------------------------------- ------- 18/22 [langchain-core]\n",
      "   -------------------------------- ------- 18/22 [langchain-core]\n",
      "   -------------------------------- ------- 18/22 [langchain-core]\n",
      "   -------------------------------- ------- 18/22 [langchain-core]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   ------------------------------------ --- 20/22 [langchain-classic]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   -------------------------------------- - 21/22 [langchain-community]\n",
      "   ---------------------------------------- 22/22 [langchain-community]\n",
      "\n",
      "Successfully installed SQLAlchemy-2.0.44 async-timeout-4.0.3 dataclasses-json-0.6.7 faiss-cpu-1.13.0 greenlet-3.2.4 httpx-sse-0.4.3 jsonpatch-1.33 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.0 langchain-text-splitters-1.0.0 langsmith-0.4.46 marshmallow-3.26.1 mypy-extensions-1.1.0 orjson-3.11.4 pydantic-settings-2.12.0 pypdf-6.4.0 python-dotenv-1.2.1 requests-toolbelt-1.0.0 sentence-transformers-5.1.2 typing-inspect-0.9.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pypdf langchain-community faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3319b371-01a8-4200-85e0-8d2b5d534580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: PDF   ...\n",
      "   .  809  .\n",
      "  (Sentence-Transformers)  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_317484\\30673361.py:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71269df9023742889cd50b8d7de3966e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9e7b511d2d43169dabce5889abd23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4716caecb5ca4a499e1115ff49d24e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cb0e6325824b6d8b9a7b235501dbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fa00b1abe54398a4e145f820e041ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAISS      ...\n",
      " RAG   ! : 'dbt_vector_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as npp \n",
    "\n",
    "# 1. \n",
    "DBT_PDF_PATH = Path(\"dbt_skills.pdf\") \n",
    "\n",
    "# 2.  DB  \n",
    "VECTOR_DB_PATH = Path(\"dbt_vector_db\") \n",
    "VECTOR_DB_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\" Step 1: PDF   ...\")\n",
    "\n",
    "if not DBT_PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\": DBT PDF ('{DBT_PDF_PATH}')   .   .\")\n",
    "\n",
    "# 3. PDF    \n",
    "loader = PyPDFLoader(str(DBT_PDF_PATH))\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,  #   \n",
    "    chunk_overlap=150\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"   .  {len(texts)}  .\")\n",
    "\n",
    "# 4.   \n",
    "print(\"  (Sentence-Transformers)  ...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 5.     \n",
    "print(\" FAISS      ...\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(str(VECTOR_DB_PATH), index_name=\"dbt_index\")\n",
    "print(f\" RAG   ! : '{VECTOR_DB_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2aba7-2ffc-4dc6-987d-88bb390b24a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff811887-12b8-4bd6-984a-25223b785f0d",
   "metadata": {},
   "source": [
    "###   !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d431af30-7ac8-4786-88d2-67c546b0f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG DB :  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- RAG/VectorDB   ---\n",
    "VECTOR_DB_PATH = Path(\"dbt_vector_db\") \n",
    "\n",
    "# @st.cache_resource   \n",
    "def load_rag_db():\n",
    "    \"\"\"DBT Vector Database .\"\"\"\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        db = FAISS.load_local(\n",
    "            str(VECTOR_DB_PATH), \n",
    "            embeddings, \n",
    "            index_name=\"dbt_index\", \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(f\"\\n RAG DB  : {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_rag_data(db, emotion: str, rationale: str):\n",
    "    \"\"\"DBT   \"\"\"\n",
    "    if db is None:\n",
    "        return \"RAG DB  \"\n",
    "        \n",
    "    query = (\n",
    "        f\" : '{emotion}'.  : '{rationale}'. \"\n",
    "        f\"     DBT   (Skills)    2 .\"\n",
    "    )\n",
    "    \n",
    "    docs = db.similarity_search(query, k=3)\n",
    "    retrieved_text = \"\\n\\n\".join([f\"--- [DBT  : {i+1}] ---\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "    \n",
    "    return retrieved_text\n",
    "\n",
    "# RAG DB  \n",
    "RAG_DB = load_rag_db()\n",
    "print(f\"RAG DB : {' ' if RAG_DB else ' '}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e8ddd39-9259-4b07-bc3a-8d60babc409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_interactive_chat_and_summarize(final_judgment_json):\n",
    "    \"\"\"\n",
    "    Step 12:      \n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" STEP 12:    \")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    final_emotion = final_judgment_json.get(\"final_emotion\", \"error\")\n",
    "    rationale = final_judgment_json.get(\"rationale\", \" \")\n",
    "    \n",
    "    # 1.    \n",
    "    chat_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    # 2.    \n",
    "    first_q = \"  **\" + final_emotion.capitalize() + \"** .  ,         ?\"\n",
    "    \n",
    "    # 3.   \n",
    "    conversation_history = [f\": {first_q}\"]\n",
    "    print(f\"\\nAI  : {first_q}\")\n",
    "    print(\"\\n(   ''  '' )\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\" : \")\n",
    "        \n",
    "        if user_input.lower() in [\"\", \"\", \"\"]:\n",
    "            break\n",
    "            \n",
    "        conversation_history.append(f\": {user_input}\")\n",
    "        \n",
    "        conversation_str = \"\\n\".join(conversation_history)\n",
    "        \n",
    "        #    \n",
    "        try:\n",
    "            chat_prompt = f\"     '  '.\\n[ ]:\\n{conversation_str}\\n\\n[ ]: {user_input}\\n\\n[   ( ):]\"\n",
    "            \n",
    "            bot_response = client.chat.completions.create(\n",
    "                model=chat_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": chat_prompt}],\n",
    "                temperature=0.7\n",
    "            ).choices[0].message.content.strip()\n",
    "            \n",
    "            conversation_history.append(f\": {bot_response}\")\n",
    "            print(f\"AI  : {bot_response}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   : {e}\")\n",
    "            break\n",
    "\n",
    "    import json\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_final_summary_report(client, emotion, rationale, chat_history, rag_context):\n",
    "    \"\"\"\n",
    "    Step 13:   JSON   LLM .\n",
    "    - , , Rationale, , RAG   .\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. RAG    \n",
    "    rag_section_prompt = \"\"\n",
    "    if rag_context and not rag_context.startswith(\"RAG DB  \") and rag_context != \" \":\n",
    "        rag_section_prompt = f\"\"\"\n",
    "        5. \"rag_recommendation\": [ DBT ]   '{emotion}'     1 ,    2-3 . (DBT    .)\n",
    "        [ DBT ]: {rag_context}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        rag_section_prompt = '5. \"rag_recommendation\": \"RAG      .\"'\n",
    "\n",
    "\n",
    "    # 2.   \n",
    "    final_summary_prompt = f\"\"\"\n",
    "        ' '.     JSON  .\n",
    "    \n",
    "    [ ]\n",
    "    1.  : {emotion}\n",
    "    2.   (Rationale): {rationale}\n",
    "    3.   : {chat_history}\n",
    "\n",
    "    []\n",
    "        5   JSON  . (date emotion    )\n",
    "\n",
    "    1. \"date\": \"{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "    2. \"final_emotion\": \"{emotion}\"\n",
    "    3. \"emotion_rationale\": \"{rationale}\"\n",
    "    4. \"chat_summary\": [  ] 2-3  . (  \"    .\" )\n",
    "    {rag_section_prompt}\n",
    "    \n",
    "    [  (JSON)]:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # GPT-4o-mini  \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": final_summary_prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        \n",
    "        return json.loads(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   LLM : {e}\")\n",
    "        return {\"error\": \"Final summary failed\"}\n",
    "\n",
    "    # 4.      (Step 13)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" STEP 13:    RAG   ...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # RAG     \n",
    "    rag_context = \"\"\n",
    "    if final_emotion.lower() in [\"sadness\", \"anger\"]:\n",
    "        #  RAG DB  ( RAG DB /!!!) \n",
    "        rag_context = retrieve_rag_data(final_emotion, rationale)\n",
    "        \n",
    "    final_report = generate_final_summary_report(\n",
    "        client=client,\n",
    "        emotion=final_emotion,\n",
    "        rationale=rationale,\n",
    "        chat_history=\"\\n\".join(conversation_history),\n",
    "        rag_context=rag_context\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\\n    !\")\n",
    "    #   \n",
    "    print(json.dumps(final_report, indent=4, ensure_ascii=False))\n",
    "    return final_report, conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc11860-40be-4ff5-9630-8622ee288fb7",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "887a63f5-ad6a-489f-a974-dd71df300ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     .\n",
      "\n",
      "==================================================\n",
      " STEP 12:    \n",
      "==================================================\n",
      "\n",
      "AI  :   **Neutral** .  ,         ?\n",
      "\n",
      "(   ''  '' )\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " :   ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI  : :  .    ,     ?          .        ?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " :         ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI  : :    .       .        ?         .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " :  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " STEP 13:    RAG   ...\n",
      "==================================================\n",
      "\n",
      "\n",
      "    !\n",
      "{\n",
      "    \"date\": \"2025-11-24\",\n",
      "    \"final_emotion\": \"neutral\",\n",
      "    \"emotion_rationale\": \"1. ** vs  :**     ''  ,       '', '' ''   .     ,       . 2. **  :**    'Hi everybody. Good morning.',     .          . 3. ** :**   ''    ''  ''    .           .       ,  ''  ,  ''    .\",\n",
      "    \"chat_summary\": \"      .    ,     .\",\n",
      "    \"rag_recommendation\": \"    .\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_final_summary_report(client, emotion, rationale, chat_history, rag_context):\n",
    "    \"\"\"\n",
    "    Step 13:   JSON   LLM . (, , Rationale, , RAG  )\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. RAG    \n",
    "    rag_section_prompt = \"\"\n",
    "    if rag_context and not rag_context.startswith(\"RAG DB  \") and rag_context != \" \":\n",
    "        rag_section_prompt = f\"\"\"\n",
    "        5. \"rag_recommendation\": [ DBT ]   '{emotion}'     1 ,    2-3 . (DBT    .)\n",
    "        [ DBT ]: {rag_context}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # RAG    \n",
    "        rag_section_prompt = '5. \"rag_recommendation\": \"    .\"'\n",
    "\n",
    "\n",
    "    # 2.    \n",
    "    final_summary_prompt = f\"\"\"\n",
    "        ' '.     JSON  .\n",
    "    \n",
    "    [ ]\n",
    "    1.  : {emotion}\n",
    "    2.   (Rationale): {rationale}\n",
    "    3.   : {chat_history}\n",
    "\n",
    "    []\n",
    "        5   JSON  . (date emotion    )\n",
    "\n",
    "    1. \"date\": \"{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "    2. \"final_emotion\": \"{emotion}\"\n",
    "    3. \"emotion_rationale\": {json.dumps(rationale)}\n",
    "    4. \"chat_summary\": [  ] 2-3  . (  \"    .\" )\n",
    "    {rag_section_prompt}\n",
    "    \n",
    "    [  (JSON)]:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # GPT-4o-mini  \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": final_summary_prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        \n",
    "        final_report_json = json.loads(content)\n",
    "        final_report_json[\"emotion_rationale\"] = rationale # Rationale  \n",
    "        final_report_json[\"date\"] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        return final_report_json\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   LLM : {e}\")\n",
    "        return {\"error\": \"Final summary failed\"}\n",
    "\n",
    "\n",
    "def start_interactive_chat_and_summarize(final_judgment_json):\n",
    "    \"\"\"\n",
    "    Step 12:      \n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" STEP 12:    \")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    final_emotion = final_judgment_json.get(\"final_emotion\", \"error\")\n",
    "    rationale = final_judgment_json.get(\"rationale\", \" \")\n",
    "    \n",
    "    chat_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    # 1.   \n",
    "    first_q = f\"  **{final_emotion.capitalize()}** .  ,         ?\"\n",
    "    \n",
    "    # 2.   \n",
    "    conversation_history = [f\": {first_q}\"]\n",
    "    print(f\"\\nAI  : {first_q}\")\n",
    "    print(\"\\n(   ''  '' )\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\" : \")\n",
    "        \n",
    "        if user_input.lower() in [\"\", \"\", \"\"]:\n",
    "            break\n",
    "            \n",
    "        conversation_history.append(f\": {user_input}\")\n",
    "        conversation_str = \"\\n\".join(conversation_history)\n",
    "        \n",
    "        #   \n",
    "        try:\n",
    "            chat_prompt = f\"     '  '.\\n[ ]:\\n{conversation_str}\\n\\n[ ]: {user_input}\\n\\n[   ( ):]\"\n",
    "            \n",
    "            bot_response = client.chat.completions.create(\n",
    "                model=chat_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": chat_prompt}],\n",
    "                temperature=0.7\n",
    "            ).choices[0].message.content.strip()\n",
    "            \n",
    "            conversation_history.append(f\": {bot_response}\")\n",
    "            print(f\"AI  : {bot_response}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"     : {e}\")\n",
    "            break\n",
    "\n",
    "    # 3.      \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" STEP 13:    RAG   ...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # RAG  \n",
    "    rag_context = \"\"\n",
    "    if final_emotion.lower() in [\"sadness\", \"anger\"]:\n",
    "        # RAG DB  \n",
    "        rag_context = retrieve_rag_data(final_emotion, rationale)\n",
    "        \n",
    "    final_report = generate_final_summary_report(\n",
    "        client=client,\n",
    "        emotion=final_emotion,\n",
    "        rationale=rationale,\n",
    "        chat_history=\"\\n\".join(conversation_history),\n",
    "        rag_context=rag_context\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\\n    !\")\n",
    "    #   \n",
    "    print(json.dumps(final_report, indent=4, ensure_ascii=False))\n",
    "    \n",
    "    return final_report, conversation_history \n",
    "\n",
    "\n",
    "# ====================================================\n",
    "#    \n",
    "# ====================================================\n",
    "\n",
    "# 1.    \n",
    "try:\n",
    "    final_judgment_dict = json.loads(result[\"final_judgment\"])\n",
    "    print(\"\\n     .\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"\\n : 'result'   . run_full_pipeline    .\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"\\n :      . -> {e}\")\n",
    "    raise\n",
    "\n",
    "# 2.    ( )\n",
    "final_report_json, saved_conversation_history = start_interactive_chat_and_summarize(final_judgment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32f781-d1c7-4fd6-8fb0-1faa68f64577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emotion_ai)",
   "language": "python",
   "name": "emotion_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
