import streamlit as st
import time
import json
import os
import uuid 
from datetime import datetime
import base64
import re
import numpy as np 

try:
    import main as backend
except ImportError:
    st.error("Error: main.py not found.")
    st.stop()

# --- ì„¤ì • ë° CSS ---
st.set_page_config(page_title="EmoDiary", layout="wide", initial_sidebar_state="collapsed")
st.markdown("""
<style>
    .stApp { background-image: linear-gradient(to bottom, #e3f2fd, #ffffff); color: black; }
    h1, h2, h3, p, div, span, label { color: black !important; font-family: 'Helvetica Neue', sans-serif; text-align: center; }
    div.stButton > button { border-radius: 30px; background-color: #4285F4 !important; color: white !important; border: none; padding: 8px 18px; width: 100%; }
    .glass-card { background: rgba(255, 255, 255, 0.9); border-radius: 15px; padding: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.1); margin-bottom: 15px; text-align: center; }
    .history-card { background: rgba(255, 255, 255, 0.95); border-radius: 15px; padding: 15px; margin-bottom: 12px; }
    .stChatInput textarea { background-color: #ffffff !important; color: #000000 !important; border: 1px solid #cccccc !important; }
    audio { display: none !important; }
</style>
""", unsafe_allow_html=True)

BASE_DIR = r"C:\Users\Owner\Desktop\SCI & ë°•ì‚¬ë…¼ë¬¸\SCI"
HISTORY_FILE = "emotion_history.json"
GIF_IDLE = os.path.join(BASE_DIR, "idle.gif")
GIF_LISTEN = os.path.join(BASE_DIR, "listening.gif")

EMO_IMAGES = {
    "happy": os.path.join(BASE_DIR, "happy.png"),
    "sad": os.path.join(BASE_DIR, "sad.png"),
    "neutral": os.path.join(BASE_DIR, "neutral.png"),
    "angry": os.path.join(BASE_DIR, "anger.png"),
    "fear": os.path.join(BASE_DIR, "fear.png"),
    "disgust": os.path.join(BASE_DIR, "disgust.png"),
    "surprise": os.path.join(BASE_DIR, "surprised.png")
}
DEFAULT_IMG = os.path.join(BASE_DIR, "neutral.png")

EMOTION_KO = {"happy": "í–‰ë³µ", "sad": "ìŠ¬í””", "neutral": "ì¤‘ë¦½", "angry": "ë¶„ë…¸", "fear": "ê³µí¬", "disgust": "í˜ì˜¤", "surprise": "ë†€ëŒ"}
VIDEO_FEAT_KO = {
    "browDownLeft": "ì™¼ìª½ ëˆˆì¹ ë‚´ë¦¼", "browDownRight": "ì˜¤ë¥¸ìª½ ëˆˆì¹ ë‚´ë¦¼",
    "mouthFrownLeft": "ì™¼ìª½ ì…ê¼¬ë¦¬ ë‚´ë¦¼", "mouthFrownRight": "ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬ ë‚´ë¦¼",
    "mouthSmileLeft": "ì™¼ìª½ ì…ê¼¬ë¦¬ ì˜¬ë¦¼", "mouthSmileRight": "ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬ ì˜¬ë¦¼"
}

def get_base64_image(file_path):
    if not os.path.exists(file_path): return ""
    with open(file_path, "rb") as f: data = f.read()
    return base64.b64encode(data).decode()

def autoplay_audio_hidden(file_path):
    if not os.path.exists(file_path): return
    with open(file_path, "rb") as f:
        data = f.read()
    st.audio(data, format="audio/mp3", autoplay=True)

def local_record_audio_only(output_wav="chat_voice.wav", duration=5, fs=16000):
    try:
        recording = backend.sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
        backend.sd.wait()
        backend.wav.write(output_wav, fs, recording)
        return output_wav
    except: return None

# --- State ---
if 'step' not in st.session_state: st.session_state['step'] = 'intro'
if 'language' not in st.session_state: st.session_state['language'] = 'ko'
if 'chat_history' not in st.session_state: st.session_state['chat_history'] = []
if 'user_emotion' not in st.session_state: st.session_state['user_emotion'] = "neutral"
if 'intro_played' not in st.session_state: st.session_state['intro_played'] = False
if 'analysis_result' not in st.session_state: st.session_state['analysis_result'] = None
if 'last_tts' not in st.session_state: st.session_state['last_tts'] = None

# ==========================================
# 1. INTRO
# ==========================================
if st.session_state['step'] == 'intro':
    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        lang_choice = st.radio("Language", ["í•œêµ­ì–´", "English"], horizontal=True, label_visibility="collapsed")
        st.session_state['language'] = 'ko' if lang_choice == "í•œêµ­ì–´" else 'en'
    with c3:
        if st.button("ğŸ“– ì¼ê¸° ë ˆí¬íŠ¸", use_container_width=True):
            st.session_state['step'] = 'history'; st.rerun()

    if os.path.exists(GIF_IDLE):
        b64 = get_base64_image(GIF_IDLE)
        st.markdown(f"""<div style="display: flex; justify-content: center; margin: 20px;"><img src="data:image/gif;base64,{b64}" width="350" style="border-radius:15px;"></div>""", unsafe_allow_html=True)
    
    greeting = "ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”? í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”." if st.session_state['language'] == 'ko' else "How was your day?"
    st.markdown(f"<h3 style='margin-bottom: 30px;'>{greeting}</h3>", unsafe_allow_html=True)

    if not st.session_state['intro_played']:
        tts_path = backend.text_to_speech(greeting, "intro_tts.mp3")
        if tts_path: autoplay_audio_hidden(tts_path); st.session_state['last_tts'] = tts_path
        st.session_state['intro_played'] = True

    b1, b2, b3 = st.columns([1, 2, 1])
    with b2:
        if st.button("ğŸ™ï¸ ëŒ€ë‹µí•˜ê¸° (ë…¹í™” ì‹œì‘)", use_container_width=True):
            st.session_state['step'] = 'recording'; st.rerun()

# ==========================================
# 2. RECORDING
# ==========================================
elif st.session_state['step'] == 'recording':
    lang = st.session_state['language']
    
    if os.path.exists(GIF_LISTEN):
        b64 = get_base64_image(GIF_LISTEN)
        st.markdown(f"""<div style="display: flex; justify-content: center; margin: 20px;"><img src="data:image/gif;base64,{b64}" width="350" style="border-radius:15px;"></div>""", unsafe_allow_html=True)
    
    msg = "ë“£ê³  ìˆì–´ìš”... ë§ì”€í•´ ì£¼ì„¸ìš”." if lang=='ko' else "Listening... Please speak."
    st.markdown(f"<h3>{msg}</h3>", unsafe_allow_html=True)

    with st.spinner("Recording..."):
        video_path = backend.record_realtime_multimodal(output_path="user_input.mp4", duration=10)
    st.session_state['video_path'] = video_path
    st.session_state['step'] = 'analysis_preview'
    st.rerun()

# ==========================================
# 3. ANALYSIS PREVIEW
# ==========================================
elif st.session_state['step'] == 'analysis_preview':
    lang = st.session_state['language']
    
    title = "ğŸ” ê°ì • ë¶„ì„ ê²°ê³¼" if lang=='ko' else "Analysis Result"
    sub = "AIê°€ ë‹¹ì‹ ì˜ ìŒì„±, í…ìŠ¤íŠ¸, í‘œì •ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤." if lang=='ko' else "AI analyzed your voice, text, and face."
    st.markdown(f"<h2>{title}</h2><p>{sub}</p>", unsafe_allow_html=True)
    
    if st.session_state.get('analysis_result') is None:
        with st.spinner("Analyzing..."):
            result = backend.run_full_pipeline(st.session_state['video_path'], language=lang)
            st.session_state['analysis_result'] = result

    res = st.session_state['analysis_result']
    details = res['details']

    speech_data = details.get('speech', {})
    speech_hyp_dict = speech_data.get('hypothesis', {})
    audio_hyp = max(speech_hyp_dict, key=speech_hyp_dict.get) if (speech_hyp_dict and 'error' not in speech_hyp_dict) else "Unknown"
    audio_evi_dict = speech_data.get('evidence', {})
    if audio_evi_dict:
        audio_evi_str = f"ìŒë†’ì´: {audio_evi_dict.get('pitch_hz', 0)}Hz, ìŒëŸ‰: {audio_evi_dict.get('vol_db', 0)}dB" if lang=='ko' else f"Pitch: {audio_evi_dict.get('pitch_hz', 0)}Hz"
    else: audio_evi_str = "N/A"

    text_data = details.get('text', {})
    text_hyp = text_data.get('hypothesis', {}).get('emotion', 'neutral')
    text_evi_str = text_data.get('evidence', {}).get('context', 'N/A')

    video_data = details.get('video', {})
    video_hyp_dict = video_data.get('hypothesis', {})
    video_hyp = max(video_hyp_dict, key=video_hyp_dict.get) if (video_hyp_dict and 'error' not in video_hyp_dict) else "neutral"
    video_evi_dict = video_data.get('evidence', {})
    if video_evi_dict:
        sorted_feats = sorted(video_evi_dict.items(), key=lambda x: x[1], reverse=True)[:2]
        video_evi_str = ", ".join([f"{VIDEO_FEAT_KO.get(k, k) if lang=='ko' else k}: {v:.2f}" for k, v in sorted_feats])
    else: video_evi_str = "N/A"

    if lang == 'ko':
        audio_hyp_ko = EMOTION_KO.get(audio_hyp, audio_hyp)
        text_hyp_ko = EMOTION_KO.get(text_hyp, text_hyp)
        video_hyp_ko = EMOTION_KO.get(str(video_hyp), str(video_hyp))
    else:
        audio_hyp_ko = audio_hyp; text_hyp_ko = text_hyp; video_hyp_ko = video_hyp

    c1, c2, c3 = st.columns(3)
    with c1: st.info(f"**ğŸ¤ ìŒì„±**\n\n**ê°€ì„¤:** {audio_hyp_ko.upper()}\n\n**ì¦ê±°:** {audio_evi_str}")
    with c2: st.info(f"**ğŸ“ í…ìŠ¤íŠ¸**\n\n**ê°€ì„¤:** {text_hyp_ko.upper()}\n\n**ì¦ê±°:** {text_evi_str}")
    with c3: st.info(f"**ğŸ“¹ ì˜ìƒ**\n\n**ê°€ì„¤:** {video_hyp_ko.upper()}\n\n**ì¦ê±°:** {video_evi_str}")

    st.write("")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        btn_txt = "ğŸ’¬ ëŒ€í™” ì‹œì‘í•˜ê¸°" if lang=='ko' else "Start Chat"
        if st.button(btn_txt, use_container_width=True):
            final_judgment = res['final_judgment']
            st.session_state['user_emotion'] = final_judgment.get('final_emotion', 'neutral')
            st.session_state['analysis_rationale'] = final_judgment.get('rationale', '')
            
            sys_prompt = f"User Emotion: {st.session_state['user_emotion']}. Context: {res['transcript']}. Start conversation warmly in Korean."
            gen_res = backend.client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "system", "content": sys_prompt}])
            first_msg = gen_res.choices[0].message.content
            
            st.session_state['chat_history'] = [{"role": "assistant", "content": first_msg}]
            unique_filename = f"tts_{uuid.uuid4().hex[:8]}.mp3"
            st.session_state['last_tts'] = backend.text_to_speech(first_msg, unique_filename)
            st.session_state['step'] = 'chatting'; st.rerun()

# ==========================================
# 4. CHATTING
# ==========================================
elif st.session_state['step'] == 'chatting':
    lang = st.session_state['language']
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if os.path.exists(GIF_IDLE):
            b64 = get_base64_image(GIF_IDLE)
            st.markdown(f"""<div class="avatar-container"><img src="data:image/gif;base64,{b64}" class="avatar-img" style="max-width: 380px;"></div>""", unsafe_allow_html=True)
        if st.session_state.get('last_tts'):
            autoplay_audio_hidden(st.session_state['last_tts']); st.session_state['last_tts'] = None

    with st.container(height=300):
        for msg in st.session_state['chat_history']:
            with st.chat_message(msg['role']): st.write(msg['content'])

    user_input = st.chat_input("ë©”ì‹œì§€ ì…ë ¥..." if lang=='ko' else "Type message...")
    c1, c2 = st.columns([2, 5])
    with c1:
        if st.button("ğŸ¤ ìŒì„±ìœ¼ë¡œ ë§í•˜ê¸°", use_container_width=True):
            with st.spinner("Listening..."):
                temp_wav = local_record_audio_only()
                if temp_wav:
                    try: 
                        wl = "ko" if lang == "ko" else "en"
                        res = backend.whisper_model.transcribe(temp_wav, language=wl, initial_prompt="ì¼ìƒ ëŒ€í™”, ê°ì • í‘œí˜„")
                        user_input = res.get("text", "")
                    except: pass
    
    if user_input:
        st.session_state['chat_history'].append({"role": "user", "content": user_input})
        ctx = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state['chat_history']])
        sys_p = f"Friend, Emotion:{st.session_state['user_emotion']}, Lang:{'Korean' if lang=='ko' else 'English'}"
        res = backend.client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"system","content":sys_p},{"role":"user","content":ctx}])
        reply = res.choices[0].message.content
        st.session_state['chat_history'].append({"role": "assistant", "content": reply})
        unique_filename = f"tts_{uuid.uuid4().hex[:8]}.mp3"
        st.session_state['last_tts'] = backend.text_to_speech(reply, unique_filename)
        st.rerun()

    if st.button("â¹ ìƒë‹´ ì¢…ë£Œ" if lang=='ko' else "Finish", use_container_width=True):
        st.session_state['step'] = 'report'; st.rerun()

# ==========================================
# 5. REPORT 
# ==========================================
elif st.session_state['step'] == 'report':
    lang = st.session_state['language']
    emotion = st.session_state['user_emotion']
    full_chat = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state['chat_history']])
    
    with st.spinner("Generating Report..."):
        rag_advice = backend.get_rag_advice(emotion, full_chat)
        
        if lang == 'ko':
            prompt = f"""
            ë‹¹ì‹ ì€ ì „ë¬¸ ì‹¬ë¦¬ ìƒë‹´ê°€ì…ë‹ˆë‹¤.
            
            [ëŒ€í™” ë‚´ìš©]
            {full_chat}
            
            [ì°¸ê³  ìë£Œ(ì „ë¬¸ ì§€ì‹)]
            {rag_advice}
            
            [ì‘ì„± ê·œì¹™]
            1. ì ˆëŒ€ ë²ˆí˜¸(1., 2.)ë‚˜ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            2. ì¤„ê¸€(Paragraph) í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
            3. ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”.

            ìš”ì•½: [ëŒ€í™” ë‚´ìš©ê³¼ ì‚¬ìš©ì ìƒí™©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½]
            
            ì¡°ì–¸: [ê°ì •({emotion})ì— ê³µê°í•˜ë©°, ìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì´ê³  ë”°ëœ»í•œ í•´ê²°ì±… 1~2ë¬¸ë‹¨]
            """
        else:
            prompt = f"""
            Summarize chat and advise for {emotion}.
            Reference: {rag_advice}
            
            [Rules]
            1. Do NOT use numbered lists. Use natural paragraphs.
            2. Format strictly as below:
            
            Summary: [Text]
            
            Advice: [Text]
            """
            
        res = backend.client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user", "content":prompt}])
        final_text = res.choices[0].message.content
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        disp_emotion = EMOTION_KO.get(emotion, emotion) if lang == 'ko' else emotion
        st.markdown(f"<div class='glass-card'><h2>Final Emotion: {disp_emotion.upper()}</h2></div>", unsafe_allow_html=True)
        
        img_key = emotion.lower()
        if 'happ' in img_key: img_key = 'happy'
        elif 'sad' in img_key: img_key = 'sad'
        elif 'ang' in img_key: img_key = 'angry'
        elif 'fear' in img_key: img_key = 'fear'
        elif 'disgust' in img_key: img_key = 'disgust'
        elif 'surprise' in img_key: img_key = 'surprise'
        elif 'neutral' in img_key: img_key = 'neutral'
        
        img_path = EMO_IMAGES.get(img_key, DEFAULT_IMG)
        if os.path.exists(img_path):
            b64 = get_base64_image(img_path)
            st.markdown(f"""<div style="display:flex; justify-content:center; margin-bottom:20px;">
                <img src='data:image/png;base64,{b64}' width='200' style="border-radius:15px;"></div>""", unsafe_allow_html=True)
        
        st.markdown(f"<div class='glass-card' style='text-align:left; white-space: pre-wrap;'>{final_text}</div>", unsafe_allow_html=True)
        
        save_txt = "ì €ì¥í•˜ê³  í™ˆìœ¼ë¡œ" if lang=='ko' else "Save & Home"
        if st.button(save_txt):
            new_record = {"date": datetime.now().strftime("%Y.%m.%d %H:%M"), "emotion": emotion, "summary": final_text, "timestamp": time.time()}
            try:
                with open(HISTORY_FILE, "r", encoding="utf-8") as f: h = json.load(f)
            except: h = []
            h.append(new_record)
            with open(HISTORY_FILE, "w", encoding="utf-8") as f: json.dump(h, f, indent=4, ensure_ascii=False)
            st.session_state['step'] = 'intro'; st.session_state['intro_played'] = False; st.session_state['analysis_result'] = None; st.session_state['chat_history'] = []; st.rerun()

# ==========================================
# 6. HISTORY
# ==========================================
elif st.session_state['step'] == 'history':
    st.markdown("<h2>ğŸ“’ ê°ì • ì¼ê¸°ì¥</h2>", unsafe_allow_html=True)
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f: data = json.load(f)
        for item in sorted(data, key=lambda x:x['timestamp'], reverse=True):
            raw_emo = item['emotion'].lower()
            final_key = raw_emo
            if 'happ' in raw_emo: final_key = 'happy'
            elif 'sad' in raw_emo: final_key = 'sad'
            elif 'ang' in raw_emo: final_key = 'angry'
            elif 'fear' in raw_emo: final_key = 'fear'
            elif 'disgust' in raw_emo: final_key = 'disgust'
            elif 'surprise' in raw_emo: final_key = 'surprise'
            elif 'neutral' in raw_emo: final_key = 'neutral'

            img_path = EMO_IMAGES.get(final_key, DEFAULT_IMG)
            img_html = ""
            if os.path.exists(img_path):
                b64 = get_base64_image(img_path)
                img_html = (f"<img src='data:image/png;base64,{b64}' width='80' style='border-radius:50%; border:2px solid #ddd;'>")
            
            display_emo = EMOTION_KO.get(final_key, final_key.upper())
            st.markdown(f"""
            <div class="history-card"><div style="display:flex; align-items:center;">
                <div style="flex:1; text-align:center;">{img_html}</div>
                <div style="flex:4; padding-left:20px;">
                    <h3 style="margin:0; font-size:1.2em; text-align:left;">{item['date']} - {display_emo}</h3>
                    <p style="margin:5px 0 0 0; font-size:0.95em; color:#555; text-align:left; white-space: pre-wrap;">{item.get('summary', 'ìš”ì•½ ì—†ìŒ')}</p>
                </div>
            </div></div>""", unsafe_allow_html=True)
    else: st.info("ê¸°ë¡ ì—†ìŒ")
    if st.button("ğŸ  í™ˆìœ¼ë¡œ"): st.session_state['step'] = 'intro'; st.rerun()